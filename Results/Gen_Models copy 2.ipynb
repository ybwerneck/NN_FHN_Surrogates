{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed problem: Bp\n",
      "aaaa       id                                             layers  total_neurons  \\\n",
      "0      0          [['ELU', 32], ['SiLU', 64], ['ReLU', 64]]            160   \n",
      "1      1         [['ReLU', 16], ['Tanh', 32], ['ELU', 128]]            176   \n",
      "2      2    [['ELU', 16], ['Sigmoid', 64], ['Sigmoid', 64]]            144   \n",
      "3      3   [['ReLU', 32], ['ReLU', 64], ['LeakyReLU', 128]]            224   \n",
      "4      4  [['LeakyReLU', 16], ['Tanh', 16], ['LeakyReLU'...             64   \n",
      "..   ...                                                ...            ...   \n",
      "795  795                       [['ELU', 128], ['ReLU', 64]]            192   \n",
      "796  796                   [['SiLU', 128], ['Sigmoid', 16]]            144   \n",
      "797  797                      [['SiLU', 128], ['ReLU', 16]]            144   \n",
      "798  798                       [['ELU', 128], ['SiLU', 16]]            144   \n",
      "799  799                        [['ELU', 32], ['Tanh', 16]]             48   \n",
      "\n",
      "          shape  \n",
      "0        funnel  \n",
      "1        funnel  \n",
      "2        funnel  \n",
      "3        funnel  \n",
      "4        funnel  \n",
      "..          ...  \n",
      "795  bottleneck  \n",
      "796  bottleneck  \n",
      "797  bottleneck  \n",
      "798  bottleneck  \n",
      "799  bottleneck  \n",
      "\n",
      "[800 rows x 4 columns]\n",
      "      id                                             layers  total_neurons  \\\n",
      "0      0          [['ELU', 32], ['SiLU', 64], ['ReLU', 64]]            160   \n",
      "1      1         [['ReLU', 16], ['Tanh', 32], ['ELU', 128]]            176   \n",
      "2      2    [['ELU', 16], ['Sigmoid', 64], ['Sigmoid', 64]]            144   \n",
      "3      3   [['ReLU', 32], ['ReLU', 64], ['LeakyReLU', 128]]            224   \n",
      "4      4  [['LeakyReLU', 16], ['Tanh', 16], ['LeakyReLU'...             64   \n",
      "..   ...                                                ...            ...   \n",
      "795  795                       [['ELU', 128], ['ReLU', 64]]            192   \n",
      "796  796                   [['SiLU', 128], ['Sigmoid', 16]]            144   \n",
      "797  797                      [['SiLU', 128], ['ReLU', 16]]            144   \n",
      "798  798                       [['ELU', 128], ['SiLU', 16]]            144   \n",
      "799  799                        [['ELU', 32], ['Tanh', 16]]             48   \n",
      "\n",
      "          shape  \n",
      "0        funnel  \n",
      "1        funnel  \n",
      "2        funnel  \n",
      "3        funnel  \n",
      "4        funnel  \n",
      "..          ...  \n",
      "795  bottleneck  \n",
      "796  bottleneck  \n",
      "797  bottleneck  \n",
      "798  bottleneck  \n",
      "799  bottleneck  \n",
      "\n",
      "[800 rows x 4 columns]     Problem         Folder  Final Error 1  Final Error 2  \\\n",
      "0        Bp  simulation252       0.304001       0.985645   \n",
      "1        Bp   simulation79       0.007150       0.628318   \n",
      "2        Bp  simulation206       0.004081       0.668939   \n",
      "3        Bp  simulation104       0.005015       0.688675   \n",
      "4        Bp   simulation71       0.007109       0.670083   \n",
      "..      ...            ...            ...            ...   \n",
      "178      Bp  simulation240       0.009482       0.688584   \n",
      "179      Bp  simulation234       0.002427       0.608215   \n",
      "180      Bp  simulation172       0.008478       0.675416   \n",
      "181      Bp   simulation13       0.010789       0.635033   \n",
      "182      Bp   simulation99       0.035262       0.664391   \n",
      "\n",
      "                                         Hidden Layers  Pinn Info  \\\n",
      "0                      [['Sigmoid', 16], ['SiLU', 32]]      False   \n",
      "1    [['ReLU', 16], ['LeakyReLU', 16], ['Sigmoid', ...       True   \n",
      "2    [['LeakyReLU', 16], ['ELU', 64], ['Sigmoid', 16]]      False   \n",
      "3      [['Tanh', 64], ['ReLU', 16], ['LeakyReLU', 16]]      False   \n",
      "4            [['SiLU', 16], ['ELU', 16], ['Tanh', 16]]       True   \n",
      "..                                                 ...        ...   \n",
      "178   [['Sigmoid', 16], ['Sigmoid', 64], ['Tanh', 16]]      False   \n",
      "179   [['LeakyReLU', 64], ['SiLU', 128], ['SiLU', 16]]      False   \n",
      "180   [['Sigmoid', 64], ['Sigmoid', 16], ['ELU', 128]]      False   \n",
      "181       [['Tanh', 16], ['Sigmoid', 16], ['ELU', 32]]       True   \n",
      "182       [['ReLU', 64], ['Sigmoid', 64], ['ELU', 64]]       True   \n",
      "\n",
      "                   Batch Size  \n",
      "0    traininig_data/treino_s/  \n",
      "1    traininig_data/treino_s/  \n",
      "2    traininig_data/treino_s/  \n",
      "3    traininig_data/treino_s/  \n",
      "4    traininig_data/treino_s/  \n",
      "..                        ...  \n",
      "178  traininig_data/treino_s/  \n",
      "179  traininig_data/treino_s/  \n",
      "180  traininig_data/treino_s/  \n",
      "181  traininig_data/treino_s/  \n",
      "182  traininig_data/treino_s/  \n",
      "\n",
      "[183 rows x 7 columns]\n",
      "    Problem         Folder  Final Error 1  Final Error 2  \\\n",
      "0        Bp  simulation252       0.304001       0.985645   \n",
      "1        Bp   simulation79       0.007150       0.628318   \n",
      "2        Bp  simulation206       0.004081       0.668939   \n",
      "3        Bp  simulation104       0.005015       0.688675   \n",
      "4        Bp   simulation71       0.007109       0.670083   \n",
      "..      ...            ...            ...            ...   \n",
      "178      Bp  simulation240       0.009482       0.688584   \n",
      "179      Bp  simulation234       0.002427       0.608215   \n",
      "180      Bp  simulation172       0.008478       0.675416   \n",
      "181      Bp   simulation13       0.010789       0.635033   \n",
      "182      Bp   simulation99       0.035262       0.664391   \n",
      "\n",
      "                                         Hidden Layers  Pinn Info  \\\n",
      "0                      [['Sigmoid', 16], ['SiLU', 32]]      False   \n",
      "1    [['ReLU', 16], ['LeakyReLU', 16], ['Sigmoid', ...       True   \n",
      "2    [['LeakyReLU', 16], ['ELU', 64], ['Sigmoid', 16]]      False   \n",
      "3      [['Tanh', 64], ['ReLU', 16], ['LeakyReLU', 16]]      False   \n",
      "4            [['SiLU', 16], ['ELU', 16], ['Tanh', 16]]       True   \n",
      "..                                                 ...        ...   \n",
      "178   [['Sigmoid', 16], ['Sigmoid', 64], ['Tanh', 16]]      False   \n",
      "179   [['LeakyReLU', 64], ['SiLU', 128], ['SiLU', 16]]      False   \n",
      "180   [['Sigmoid', 64], ['Sigmoid', 16], ['ELU', 128]]      False   \n",
      "181       [['Tanh', 16], ['Sigmoid', 16], ['ELU', 32]]       True   \n",
      "182       [['ReLU', 64], ['Sigmoid', 64], ['ELU', 64]]       True   \n",
      "\n",
      "                   Batch Size  \n",
      "0    traininig_data/treino_s/  \n",
      "1    traininig_data/treino_s/  \n",
      "2    traininig_data/treino_s/  \n",
      "3    traininig_data/treino_s/  \n",
      "4    traininig_data/treino_s/  \n",
      "..                        ...  \n",
      "178  traininig_data/treino_s/  \n",
      "179  traininig_data/treino_s/  \n",
      "180  traininig_data/treino_s/  \n",
      "181  traininig_data/treino_s/  \n",
      "182  traininig_data/treino_s/  \n",
      "\n",
      "[183 rows x 7 columns]\n",
      "    Problem         Folder  Final Error 1  Final Error 2  \\\n",
      "0        Bp  simulation252       0.304001       0.985645   \n",
      "1        Bp   simulation79       0.007150       0.628318   \n",
      "2        Bp  simulation206       0.004081       0.668939   \n",
      "3        Bp  simulation104       0.005015       0.688675   \n",
      "4        Bp   simulation71       0.007109       0.670083   \n",
      "..      ...            ...            ...            ...   \n",
      "178      Bp  simulation240       0.009482       0.688584   \n",
      "179      Bp  simulation234       0.002427       0.608215   \n",
      "180      Bp  simulation172       0.008478       0.675416   \n",
      "181      Bp   simulation13       0.010789       0.635033   \n",
      "182      Bp   simulation99       0.035262       0.664391   \n",
      "\n",
      "                                         Hidden Layers  Pinn Info  \\\n",
      "0                      [['Sigmoid', 16], ['SiLU', 32]]      False   \n",
      "1    [['ReLU', 16], ['LeakyReLU', 16], ['Sigmoid', ...       True   \n",
      "2    [['LeakyReLU', 16], ['ELU', 64], ['Sigmoid', 16]]      False   \n",
      "3      [['Tanh', 64], ['ReLU', 16], ['LeakyReLU', 16]]      False   \n",
      "4            [['SiLU', 16], ['ELU', 16], ['Tanh', 16]]       True   \n",
      "..                                                 ...        ...   \n",
      "178   [['Sigmoid', 16], ['Sigmoid', 64], ['Tanh', 16]]      False   \n",
      "179   [['LeakyReLU', 64], ['SiLU', 128], ['SiLU', 16]]      False   \n",
      "180   [['Sigmoid', 64], ['Sigmoid', 16], ['ELU', 128]]      False   \n",
      "181       [['Tanh', 16], ['Sigmoid', 16], ['ELU', 32]]       True   \n",
      "182       [['ReLU', 64], ['Sigmoid', 64], ['ELU', 64]]       True   \n",
      "\n",
      "                   Batch Size  id layers  total_neurons shape  \n",
      "0    traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "1    traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "2    traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "3    traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "4    traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "..                        ...  ..    ...            ...   ...  \n",
      "178  traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "179  traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "180  traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "181  traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "182  traininig_data/treino_s/ NaN    NaN            NaN   NaN  \n",
      "\n",
      "[183 rows x 11 columns]\n",
      "Final combined CSV saved to all.csv\n",
      "    Problem         Folder  Final Error 1  Final Error 2  \\\n",
      "0        Bp  simulation252       0.304001       0.985645   \n",
      "1        Bp   simulation79       0.007150       0.628318   \n",
      "2        Bp  simulation206       0.004081       0.668939   \n",
      "3        Bp  simulation104       0.005015       0.688675   \n",
      "4        Bp   simulation71       0.007109       0.670083   \n",
      "..      ...            ...            ...            ...   \n",
      "178      Bp  simulation240       0.009482       0.688584   \n",
      "179      Bp  simulation234       0.002427       0.608215   \n",
      "180      Bp  simulation172       0.008478       0.675416   \n",
      "181      Bp   simulation13       0.010789       0.635033   \n",
      "182      Bp   simulation99       0.035262       0.664391   \n",
      "\n",
      "                                         Hidden Layers  Pinn Info  \\\n",
      "0                      [['Sigmoid', 16], ['SiLU', 32]]      False   \n",
      "1    [['ReLU', 16], ['LeakyReLU', 16], ['Sigmoid', ...       True   \n",
      "2    [['LeakyReLU', 16], ['ELU', 64], ['Sigmoid', 16]]      False   \n",
      "3      [['Tanh', 64], ['ReLU', 16], ['LeakyReLU', 16]]      False   \n",
      "4            [['SiLU', 16], ['ELU', 16], ['Tanh', 16]]       True   \n",
      "..                                                 ...        ...   \n",
      "178   [['Sigmoid', 16], ['Sigmoid', 64], ['Tanh', 16]]      False   \n",
      "179   [['LeakyReLU', 64], ['SiLU', 128], ['SiLU', 16]]      False   \n",
      "180   [['Sigmoid', 64], ['Sigmoid', 16], ['ELU', 128]]      False   \n",
      "181       [['Tanh', 16], ['Sigmoid', 16], ['ELU', 32]]       True   \n",
      "182       [['ReLU', 64], ['Sigmoid', 64], ['ELU', 64]]       True   \n",
      "\n",
      "                   Batch Size  total_neurons shape  \n",
      "0    traininig_data/treino_s/            NaN   NaN  \n",
      "1    traininig_data/treino_s/            NaN   NaN  \n",
      "2    traininig_data/treino_s/            NaN   NaN  \n",
      "3    traininig_data/treino_s/            NaN   NaN  \n",
      "4    traininig_data/treino_s/            NaN   NaN  \n",
      "..                        ...            ...   ...  \n",
      "178  traininig_data/treino_s/            NaN   NaN  \n",
      "179  traininig_data/treino_s/            NaN   NaN  \n",
      "180  traininig_data/treino_s/            NaN   NaN  \n",
      "181  traininig_data/treino_s/            NaN   NaN  \n",
      "182  traininig_data/treino_s/            NaN   NaN  \n",
      "\n",
      "[183 rows x 9 columns]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def custom_merge(df, reference_df, key_df=\"Hidden Layers\", key_ref=\"layers\"):\n",
    "    \"\"\"\n",
    "    Custom merge function to handle lists in DataFrame columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The main DataFrame to enrich.\n",
    "        reference_df (pd.DataFrame): The reference DataFrame with additional information.\n",
    "        key_df (str): The column name in df to match.\n",
    "        key_ref (str): The column name in reference_df to match.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The enriched DataFrame.\n",
    "    \"\"\"\n",
    "    print(df)\n",
    "    # Use a helper function to normalize lists for comparison\n",
    "    def normalize(value):\n",
    "\n",
    "            return value\n",
    "\n",
    "    # Create normalized keys for both DataFrames\n",
    "    df[\"_merge_key\"] = df[key_df].apply(normalize)\n",
    "    reference_df[\"_merge_key\"] = reference_df[key_ref].apply(normalize)\n",
    "\n",
    "    # Perform the merge on the normalized keys\n",
    "    merged_df = df.merge(reference_df, left_on=\"_merge_key\", right_on=\"_merge_key\", how=\"left\").drop(columns=[\"_merge_key\"])\n",
    "\n",
    "    return merged_df\n",
    "def enrich_with_reference(df, reference_df):\n",
    "    \"\"\"\n",
    "    Enrich a DataFrame with additional information from a reference DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to enrich.\n",
    "        reference_df (pd.DataFrame): The reference DataFrame with additional information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The enriched DataFrame.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Merge based on the 'Hidden Layers' field\n",
    "    print(\"aaaa\",reference_df)\n",
    "    print(reference_df,df)\n",
    "    enriched_df = custom_merge(df,reference_df)\n",
    "    print(enriched_df)\n",
    "    return enriched_df\n",
    "\n",
    "def process_folder_to_csv(base_dir, output_csv_path,Prob=\"--\"):\n",
    "    \"\"\"\n",
    "    Process all simulation folders in the base directory and save results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir (str): Path to the base directory containing simulation folders.\n",
    "        output_csv_path (str): Path to save the resulting CSV file.\n",
    "    \"\"\"\n",
    "    columns = [\"Folder\", \"Final Error 1\", \"Final Error 2\", \"Hidden Layers\", \"Pinn Info\", \"Batch Size\",\"Problem\"]\n",
    "    results_table = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Iterate through each subfolder\n",
    "    for subdir in os.listdir(base_dir):\n",
    "        try:\n",
    "            if os.path.isdir(os.path.join(base_dir, subdir)) and subdir.startswith('simulation'):\n",
    "                h5_file_path = os.path.join(base_dir, subdir, 'val_err.h5')\n",
    "                pickle_file_path = os.path.join(base_dir, subdir, 'my_dict.pkl')\n",
    "\n",
    "                if os.path.isfile(h5_file_path):\n",
    "                    with h5py.File(h5_file_path, 'r') as f:\n",
    "                        print(f\"Processing folder: {subdir}\")\n",
    "                        learning_curve = np.array(f['error_stats'])\n",
    "                        try:\n",
    "                            final_value1 = learning_curve.T[0][-1]\n",
    "                            final_value2 = learning_curve.T[1][-1]\n",
    "                        except IndexError:\n",
    "                            final_value1, final_value2 = np.nan, np.nan\n",
    "\n",
    "                # Extract model information from pickle file\n",
    "                if os.path.isfile(pickle_file_path):\n",
    "                    with open(pickle_file_path, 'rb') as pf:\n",
    "                        sim_data = pickle.load(pf)\n",
    "\n",
    "                    model_info = sim_data.get('model_params', {})\n",
    "                    pinn = model_info.get('pinn', \"N/A\")\n",
    "                    bs = model_info.get('bs', \"N/A\")\n",
    "                    hidden_layers = model_info.get('hidden_layers', \"N/A\")\n",
    "                else:\n",
    "                    pinn = \"N/A\"\n",
    "                    bs = \"N/A\"\n",
    "                    hidden_layers = \"N/A\"\n",
    "\n",
    "                # Add the extracted data to the DataFrame\n",
    "                results_table.loc[len(results_table)] = [\n",
    "                    subdir, final_value1, final_value2, hidden_layers, pinn, bs,Prob\n",
    "                ]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping folder {subdir} due to an error: {e}\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    print(\"aasdas\")\n",
    "    \n",
    "    results=enrich_with_reference(results,pd.read_csv(\"model_zoo.csv\"))\n",
    "    results_table.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "def combine_csv_files(csv_files, combined_csv_path):\n",
    "    \"\"\"\n",
    "    Combine multiple CSV files into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        csv_files (list of str): List of paths to the CSV files to combine.\n",
    "        combined_csv_path (str): Path to save the combined CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            print(f\"Reading file: {csv_file}\")\n",
    "            df = pd.read_csv(csv_file)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {csv_file} due to an error: {e}\")\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Combined results saved to {combined_csv_path}\")\n",
    "    return combined_df\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def process_and_combine_from_dict(input_dict, output_csv_path):\n",
    "    \"\"\"\n",
    "    Processes subdirectories from a dictionary and concatenates the results into a single CSV.\n",
    "\n",
    "    Parameters:\n",
    "        input_dict (dict): Dictionary where keys are problem names and values are subdirectory paths.\n",
    "        output_csv_path (str): Path to save the final concatenated CSV file.\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Process each problem and subdirectory\n",
    "    for problem_name, subdir in input_dict.items():\n",
    "        try:\n",
    "            # Initialize a DataFrame for this problem\n",
    "            columns = [\"Problem\", \"Folder\", \"Final Error 1\", \"Final Error 2\", \"Hidden Layers\", \"Pinn Info\", \"Batch Size\"]\n",
    "            results_table = pd.DataFrame(columns=columns)\n",
    "\n",
    "            # Iterate through simulation folders\n",
    "            for folder in os.listdir(subdir):\n",
    "                folder_path = os.path.join(subdir, folder)\n",
    "                if os.path.isdir(folder_path) and folder.startswith('simulation'):\n",
    "                    h5_file_path = os.path.join(folder_path, 'val_err.h5')\n",
    "                    pickle_file_path = os.path.join(folder_path, 'my_dict.pkl')\n",
    "\n",
    "                    # Extract learning curve data\n",
    "                try:\n",
    "                    if os.path.isfile(h5_file_path):\n",
    "                        with h5py.File(h5_file_path, 'r') as f:\n",
    "                            learning_curve = np.array(f['error_stats'])\n",
    "                            try:\n",
    "                                final_value1 = learning_curve.T[0][-1]\n",
    "                                final_value2 = learning_curve.T[1][-1]\n",
    "                            except IndexError:\n",
    "                                final_value1, final_value2 = np.nan, np.nan\n",
    "                    else:\n",
    "                        final_value1, final_value2 = np.nan, np.nan\n",
    "   \n",
    "                    # Extract model info from pickle\n",
    "                    if os.path.isfile(pickle_file_path):\n",
    "                        with open(pickle_file_path, 'rb') as pf:\n",
    "                            sim_data = pickle.load(pf)\n",
    "                            model_info = sim_data.get('model_params', {})\n",
    "                           # print(sim_data)\n",
    "                           # print(model_info)\n",
    "                            pinn = model_info.get('pinn', \"N/A\")\n",
    "            \n",
    "                            hidden_layers=str([[str(a), b] for  a,b  in model_info.get('hidden_layers', \"N/A\")])\n",
    "                            bs=model_info.get('training_set',\"N/A\")\n",
    "                    else:\n",
    "                        pinn = \"N/A\"\n",
    "                        bs = \"N/A\"\n",
    "                        hidden_layers = \"N/A\"\n",
    "\n",
    "                    # Append data to results table\n",
    "                    results_table.loc[len(results_table)] = [\n",
    "                        problem_name, folder, final_value1, final_value2, hidden_layers, pinn, bs\n",
    "                    ]\n",
    "                except Exception as e :\n",
    "                     print(\"e\",e)\n",
    "            # Concatenate results for this problem into the combined DataFrame\n",
    "            combined_df = pd.concat([combined_df, results_table], ignore_index=True)\n",
    "            print(f\"Processed problem: {problem_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping problem {problem_name} due to an error: {e}\")\n",
    "\n",
    "    # Save the final combined CSV\n",
    "    combined_df=enrich_with_reference(combined_df,pd.read_csv(\"model_zoo.csv\"))\n",
    "    combined_df=combined_df.drop(columns=[\"layers\",\"id\"])\n",
    "    combined_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Final combined CSV saved to {output_csv_path}\")\n",
    "    return combined_df\n",
    "# Example input dictionary\n",
    "input_dict = {\n",
    "    \"Bp\": \"../Problem_B/Problem_B_Results/\",\n",
    "}\n",
    "\n",
    "# Call the function\n",
    "models=process_and_combine_from_dict(input_dict, \"all.csv\")\n",
    "\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Problem         Folder  Final Error 1  Final Error 2  \\\n",
      "0        Bp   simulation79       0.007150       0.628318   \n",
      "1        Bp  simulation206       0.004007       0.708595   \n",
      "2        Bp  simulation104       0.005015       0.688675   \n",
      "3        Bp   simulation71       0.007179       0.613609   \n",
      "4        Bp  simulation178       0.008304       0.796254   \n",
      "..      ...            ...            ...            ...   \n",
      "163      Bp   simulation27       0.011551       0.696851   \n",
      "164      Bp   simulation17       0.009724       0.839701   \n",
      "165      Bp  simulation172       0.008478       0.675416   \n",
      "166      Bp   simulation13       0.010789       0.635033   \n",
      "167      Bp   simulation99       0.381505       1.087251   \n",
      "\n",
      "                                         Hidden Layers  Pinn Info  \\\n",
      "0    [['ReLU', 16], ['LeakyReLU', 16], ['Sigmoid', ...       True   \n",
      "1    [['LeakyReLU', 16], ['ELU', 64], ['Sigmoid', 16]]      False   \n",
      "2      [['Tanh', 64], ['ReLU', 16], ['LeakyReLU', 16]]      False   \n",
      "3            [['SiLU', 16], ['ELU', 16], ['Tanh', 16]]       True   \n",
      "4    [['ReLU', 32], ['LeakyReLU', 16], ['Sigmoid', ...      False   \n",
      "..                                                 ...        ...   \n",
      "163       [['Sigmoid', 16], ['Tanh', 16], ['ELU', 32]]       True   \n",
      "164    [['LeakyReLU', 16], ['SiLU', 32], ['SiLU', 32]]       True   \n",
      "165   [['Sigmoid', 64], ['Sigmoid', 16], ['ELU', 128]]      False   \n",
      "166       [['Tanh', 16], ['Sigmoid', 16], ['ELU', 32]]       True   \n",
      "167       [['ReLU', 64], ['Sigmoid', 64], ['ELU', 64]]       True   \n",
      "\n",
      "                   Batch Size  total_neurons shape  \n",
      "0    traininig_data/treino_s/            NaN   NaN  \n",
      "1    traininig_data/treino_s/            NaN   NaN  \n",
      "2    traininig_data/treino_s/            NaN   NaN  \n",
      "3    traininig_data/treino_s/            NaN   NaN  \n",
      "4    traininig_data/treino_s/            NaN   NaN  \n",
      "..                        ...            ...   ...  \n",
      "163  traininig_data/treino_s/            NaN   NaN  \n",
      "164  traininig_data/treino_s/            NaN   NaN  \n",
      "165  traininig_data/treino_s/            NaN   NaN  \n",
      "166  traininig_data/treino_s/            NaN   NaN  \n",
      "167  traininig_data/treino_s/            NaN   NaN  \n",
      "\n",
      "[168 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tanh', 64], ['ReLU', 16], ['LeakyReLU', 16]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['Tanh', 64], ['ELU', 16]]\n",
      "e\n",
      "[['Tanh', 64], ['SiLU', 16], ['ELU', 16]]\n",
      "e\n",
      "[['ReLU', 16], ['SiLU', 16], ['SiLU', 32]]\n",
      "e\n",
      "[['Sigmoid', 64], ['Sigmoid', 32], ['SiLU', 16]]\n",
      "e\n",
      "[['SiLU', 16], ['ELU', 32], ['ReLU', 32]]\n",
      "e\n",
      "[['ReLU', 64], ['LeakyReLU', 32], ['LeakyReLU', 16]]\n",
      "e\n",
      "[['Tanh', 16], ['ELU', 16], ['LeakyReLU', 128]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['Tanh', 64], ['SiLU', 32]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['ELU', 64], ['Sigmoid', 16]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['ELU', 64], ['LeakyReLU', 16]]\n",
      "e\n",
      "[['ReLU', 32], ['ELU', 32], ['Tanh', 128]]\n",
      "e\n",
      "[['SiLU', 32], ['Tanh', 32], ['Tanh', 32]]\n",
      "e\n",
      "[['SiLU', 16], ['ELU', 32], ['ReLU', 32]]\n",
      "e\n",
      "[['Sigmoid', 16], ['SiLU', 16], ['LeakyReLU', 16]]\n",
      "e\n",
      "[['ELU', 64], ['Sigmoid', 32], ['Sigmoid', 16]]\n",
      "e\n",
      "[['ReLU', 16], ['ELU', 16], ['LeakyReLU', 128]]\n",
      "e\n",
      "[['LeakyReLU', 128], ['ReLU', 32], ['SiLU', 32]]\n",
      "e\n",
      "[['Tanh', 16], ['LeakyReLU', 16], ['LeakyReLU', 64]]\n",
      "e\n",
      "[['ReLU', 16], ['SiLU', 16], ['SiLU', 32]]\n",
      "e\n",
      "[['ReLU', 64], ['Sigmoid', 64], ['SiLU', 64]]\n",
      "e\n",
      "[['ReLU', 64], ['Sigmoid', 64], ['ELU', 64]]\n",
      "e\n",
      "[['Tanh', 16], ['LeakyReLU', 16], ['LeakyReLU', 64]]\n",
      "e\n",
      "[['SiLU', 16], ['ELU', 32], ['ReLU', 64]]\n",
      "e\n",
      "[['SiLU', 32], ['Tanh', 32], ['Tanh', 32]]\n",
      "e\n",
      "[['Tanh', 16], ['ReLU', 16], ['ELU', 16]]\n",
      "e\n",
      "[['ELU', 16], ['LeakyReLU', 16], ['Sigmoid', 16]]\n",
      "e\n",
      "[['SiLU', 16], ['ReLU', 16], ['Sigmoid', 64]]\n",
      "e\n",
      "[['ELU', 16], ['SiLU', 16], ['SiLU', 16]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['LeakyReLU', 64], ['SiLU', 64]]\n",
      "e\n",
      "[['Tanh', 16], ['SiLU', 32], ['SiLU', 128]]\n",
      "e\n",
      "[['SiLU', 16], ['ReLU', 16], ['Sigmoid', 64]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['LeakyReLU', 64], ['ReLU', 64]]\n",
      "e\n",
      "[['ELU', 32], ['Sigmoid', 32], ['LeakyReLU', 16]]\n",
      "e\n",
      "[['LeakyReLU', 16], ['SiLU', 16], ['LeakyReLU', 64]]\n",
      "e\n",
      "[['Tanh', 16], ['ELU', 32], ['ReLU', 64]]\n",
      "e\n",
      "[['ELU', 32], ['ELU', 64], ['Tanh', 64]]\n",
      "e\n",
      "[['ELU', 16], ['Tanh', 64], ['Sigmoid', 64]]\n",
      "e\n",
      "[['LeakyReLU', 32], ['Tanh', 32], ['SiLU', 16]]\n",
      "e\n",
      "[['Tanh', 32], ['Sigmoid', 16], ['Sigmoid', 16]]\n",
      "e\n",
      "[['SiLU', 16], ['ELU', 16], ['Tanh', 16]]\n",
      "e\n",
      "[['ELU', 16], ['Tanh', 16], ['ReLU', 16]]\n",
      "e\n",
      "[['Tanh', 16], ['Sigmoid', 16], ['ELU', 32]]\n",
      "e\n",
      "[['Tanh', 128], ['ELU', 64], ['Tanh', 64]]\n",
      "e\n",
      "[['SiLU', 64], ['Sigmoid', 64], ['SiLU', 64]]\n",
      "e\n",
      "[['Tanh', 64], ['Tanh', 16], ['LeakyReLU', 16]]\n",
      "e\n",
      "[['ReLU', 64], ['Tanh', 16], ['Tanh', 128]]\n",
      "e\n",
      "[['Sigmoid', 16], ['ELU', 64], ['LeakyReLU', 128]]\n",
      "e\n",
      "[['ELU', 32], ['Tanh', 32], ['Tanh', 16]]\n",
      "e\n",
      "[['Sigmoid', 16], ['SiLU', 32], ['Tanh', 128]]\n",
      "e\n",
      "[['Sigmoid', 32], ['ReLU', 32], ['Tanh', 128]]\n",
      "e\n",
      "[['Tanh', 16], ['SiLU', 32], ['SiLU', 128]]\n",
      "e\n",
      "[['Tanh', 16], ['ELU', 16], ['LeakyReLU', 128]]\n",
      "e\n",
      "[['SiLU', 16], ['SiLU', 64], ['Sigmoid', 64]]\n",
      "e\n",
      "[['ReLU', 16], ['SiLU', 16], ['SiLU', 16]]\n",
      "e\n",
      "[['Sigmoid', 32], ['SiLU', 32], ['Tanh', 32]]\n",
      "e\n",
      "[['ELU', 16], ['SiLU', 16], ['SiLU', 16]]\n",
      "e\n",
      "[['ReLU', 128], ['ReLU', 64], ['SiLU', 64]]\n",
      "e\n",
      "[['Sigmoid', 16], ['ELU', 16], ['ReLU', 32]]\n",
      "e\n",
      "[['SiLU', 16], ['SiLU', 64], ['Sigmoid', 64]]\n",
      "e\n",
      "[['ELU', 128], ['LeakyReLU', 64], ['Sigmoid', 16]]\n",
      "e\n",
      "[['ReLU', 16], ['LeakyReLU', 16], ['Sigmoid', 16]]\n",
      "e\n",
      "[['LeakyReLU', 32], ['ReLU', 32], ['LeakyReLU', 32]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['LeakyReLU', 64], ['SiLU', 64]]\n",
      "e\n",
      "[['Sigmoid', 32], ['ReLU', 32], ['Tanh', 128]]\n",
      "e\n",
      "[['SiLU', 64], ['SiLU', 64], ['SiLU', 32]]\n",
      "e\n",
      "[['SiLU', 16], ['ELU', 16], ['ELU', 32]]\n",
      "e\n",
      "[['Sigmoid', 64], ['SiLU', 64], ['ELU', 64]]\n",
      "e\n",
      "[['Sigmoid', 32], ['SiLU', 32], ['Tanh', 32]]\n",
      "e\n",
      "[['SiLU', 32], ['ELU', 32], ['SiLU', 32]]\n",
      "e\n",
      "[['ELU', 64], ['ReLU', 16], ['ReLU', 16]]\n",
      "e\n",
      "[['SiLU', 16], ['ELU', 16], ['ELU', 32]]\n",
      "e\n",
      "[['Sigmoid', 32], ['ELU', 32], ['ReLU', 64]]\n",
      "e\n",
      "[['Sigmoid', 32], ['ELU', 32], ['ReLU', 64]]\n",
      "e\n",
      "[['ELU', 128], ['Tanh', 64], ['ReLU', 16]]\n",
      "e\n",
      "[['ReLU', 64], ['Tanh', 64], ['ReLU', 64]]\n",
      "e\n",
      "[['ReLU', 32], ['ELU', 32], ['Tanh', 128]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['Tanh', 64], ['ReLU', 64]]\n",
      "e\n",
      "[['SiLU', 16], ['ELU', 32], ['ReLU', 64]]\n",
      "e\n",
      "[['LeakyReLU', 16], ['SiLU', 16], ['LeakyReLU', 64]]\n",
      "e\n",
      "[['Tanh', 16], ['ELU', 32], ['ReLU', 64]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['ELU', 32], ['LeakyReLU', 32]]\n",
      "e\n",
      "[['SiLU', 64], ['Sigmoid', 64], ['SiLU', 64]]\n",
      "e\n",
      "[['LeakyReLU', 16], ['SiLU', 32], ['SiLU', 32]]\n",
      "e\n",
      "[['LeakyReLU', 128], ['Sigmoid', 64], ['Tanh', 32]]\n",
      "e\n",
      "[['LeakyReLU', 16], ['Tanh', 16], ['ReLU', 128]]\n",
      "e\n",
      "[['ReLU', 64], ['Tanh', 64], ['ReLU', 64]]\n",
      "e\n",
      "[['Sigmoid', 16], ['SiLU', 32], ['Tanh', 128]]\n",
      "e\n",
      "[['Sigmoid', 16], ['ELU', 64], ['LeakyReLU', 128]]\n",
      "e\n",
      "[['Sigmoid', 16], ['SiLU', 16], ['LeakyReLU', 16]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['LeakyReLU', 64], ['ReLU', 64]]\n",
      "e\n",
      "[['LeakyReLU', 128], ['Sigmoid', 64], ['SiLU', 16]]\n",
      "e\n",
      "[['ELU', 64], ['ELU', 64], ['Sigmoid', 64]]\n",
      "e\n",
      "[['ELU', 16], ['Tanh', 64], ['Sigmoid', 64]]\n",
      "e\n",
      "[['Sigmoid', 16], ['ELU', 16], ['ReLU', 32]]\n",
      "e\n",
      "[['ELU', 16], ['LeakyReLU', 16], ['Tanh', 16]]\n",
      "e\n",
      "[['LeakyReLU', 64], ['Tanh', 64], ['ReLU', 64]]\n",
      "e\n",
      "[['SiLU', 128], ['Tanh', 64], ['Sigmoid', 64]]\n",
      "e\n",
      "[['ELU', 32], ['ELU', 64], ['Tanh', 64]]\n",
      "e\n",
      "[['SiLU', 32], ['Tanh', 32], ['ReLU', 32]]\n",
      "e\n",
      "[['Tanh', 32], ['Tanh', 32], ['SiLU', 32]]\n",
      "e\n",
      "[['Tanh', 16], ['LeakyReLU', 16], ['Tanh', 16]]\n",
      "e\n",
      "[['Sigmoid', 16], ['Tanh', 16], ['ELU', 32]]\n",
      "e\n",
      "[['LeakyReLU', 16], ['Tanh', 16], ['ReLU', 128]]\n",
      "e\n",
      "[['ReLU', 16], ['Sigmoid', 32], ['ReLU', 128]]\n",
      "e\n",
      "[['ReLU', 16], ['ELU', 16], ['LeakyReLU', 128]]\n",
      "e\n",
      "[['ReLU', 16], ['Sigmoid', 32], ['ReLU', 128]]\n",
      "e\n",
      "[['Sigmoid', 16], ['Tanh', 16], ['ELU', 32]]\n",
      "e\n",
      "[['LeakyReLU', 16], ['SiLU', 32], ['SiLU', 32]]\n",
      "e\n",
      "[['Tanh', 16], ['Sigmoid', 16], ['ELU', 32]]\n",
      "e\n",
      "Processed problem: Bp\n",
      "aaaa       id                                             layers  total_neurons  \\\n",
      "0      0          [['ELU', 32], ['SiLU', 64], ['ReLU', 64]]            160   \n",
      "1      1         [['ReLU', 16], ['Tanh', 32], ['ELU', 128]]            176   \n",
      "2      2    [['ELU', 16], ['Sigmoid', 64], ['Sigmoid', 64]]            144   \n",
      "3      3   [['ReLU', 32], ['ReLU', 64], ['LeakyReLU', 128]]            224   \n",
      "4      4  [['LeakyReLU', 16], ['Tanh', 16], ['LeakyReLU'...             64   \n",
      "..   ...                                                ...            ...   \n",
      "795  795                       [['ELU', 128], ['ReLU', 64]]            192   \n",
      "796  796                   [['SiLU', 128], ['Sigmoid', 16]]            144   \n",
      "797  797                      [['SiLU', 128], ['ReLU', 16]]            144   \n",
      "798  798                       [['ELU', 128], ['SiLU', 16]]            144   \n",
      "799  799                        [['ELU', 32], ['Tanh', 16]]             48   \n",
      "\n",
      "          shape  \n",
      "0        funnel  \n",
      "1        funnel  \n",
      "2        funnel  \n",
      "3        funnel  \n",
      "4        funnel  \n",
      "..          ...  \n",
      "795  bottleneck  \n",
      "796  bottleneck  \n",
      "797  bottleneck  \n",
      "798  bottleneck  \n",
      "799  bottleneck  \n",
      "\n",
      "[800 rows x 4 columns]\n",
      "      id                                             layers  total_neurons  \\\n",
      "0      0          [['ELU', 32], ['SiLU', 64], ['ReLU', 64]]            160   \n",
      "1      1         [['ReLU', 16], ['Tanh', 32], ['ELU', 128]]            176   \n",
      "2      2    [['ELU', 16], ['Sigmoid', 64], ['Sigmoid', 64]]            144   \n",
      "3      3   [['ReLU', 32], ['ReLU', 64], ['LeakyReLU', 128]]            224   \n",
      "4      4  [['LeakyReLU', 16], ['Tanh', 16], ['LeakyReLU'...             64   \n",
      "..   ...                                                ...            ...   \n",
      "795  795                       [['ELU', 128], ['ReLU', 64]]            192   \n",
      "796  796                   [['SiLU', 128], ['Sigmoid', 16]]            144   \n",
      "797  797                      [['SiLU', 128], ['ReLU', 16]]            144   \n",
      "798  798                       [['ELU', 128], ['SiLU', 16]]            144   \n",
      "799  799                        [['ELU', 32], ['Tanh', 16]]             48   \n",
      "\n",
      "          shape  \n",
      "0        funnel  \n",
      "1        funnel  \n",
      "2        funnel  \n",
      "3        funnel  \n",
      "4        funnel  \n",
      "..          ...  \n",
      "795  bottleneck  \n",
      "796  bottleneck  \n",
      "797  bottleneck  \n",
      "798  bottleneck  \n",
      "799  bottleneck  \n",
      "\n",
      "[800 rows x 4 columns] Empty DataFrame\n",
      "Columns: [Problem, Folder, Final Error 1, Final Error 2, Hidden Layers, Pinn Info, Batch Size]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Problem, Folder, Final Error 1, Final Error 2, Hidden Layers, Pinn Info, Batch Size]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Problem, Folder, Final Error 1, Final Error 2, Hidden Layers, Pinn Info, Batch Size, id, layers, total_neurons, shape]\n",
      "Index: []\n",
      "Final combined CSV saved to all.csv\n",
      "Empty DataFrame\n",
      "Columns: [Problem, Folder, Final Error 1, Final Error 2, Hidden Layers, Pinn Info, Batch Size, total_neurons, shape]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_errors_vs_neurons(df, error_col, neuron_col, shape_col, pinn_col):\n",
    "    \"\"\"\n",
    "    Plots model errors against total neurons, creating separate plots for each shape.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        error_col (str): Column name for error values.\n",
    "        neuron_col (str): Column name for total neurons.\n",
    "        shape_col (str): Column name for model shapes.\n",
    "        pinn_col (str): Column name indicating PINN info (True/False).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    unique_shapes = df[shape_col].unique()\n",
    "    colors = df[pinn_col].map({True: 'blue', False: 'orange'})\n",
    "\n",
    "    # Create separate plots for each shape\n",
    "    for shape in unique_shapes:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        shape_mask = df[shape_col] == shape\n",
    "        \n",
    "        plt.scatter(\n",
    "            df[neuron_col][shape_mask], df[error_col][shape_mask],\n",
    "            c=colors[shape_mask],\n",
    "            s=25,  # Marker size\n",
    "            label=f'Shape: {shape}'\n",
    "        )\n",
    "\n",
    "        plt.title(f'Model Errors vs. Total Neurons ({shape})', fontsize=14)\n",
    "        plt.xlabel('Total Neurons', fontsize=12)\n",
    "        plt.ylabel('Final Error 1', fontsize=12)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.ylim(1e-3,1e-1)\n",
    "        plt.legend(title=\"PINN Info\", fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming `df` is your DataFrame\n",
    "plot_model_errors_vs_neurons(models, error_col=\"Final Error 1\", neuron_col=\"total_neurons\", shape_col=\"Batch Size\", pinn_col=\"Pinn Info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `models` is your DataFrame\n",
    "median_errors = models.groupby([\"Pinn Info\", \"Batch Size\"])[\"Final Error 1\"].median()\n",
    "\n",
    "# Print the result\n",
    "print(median_errors)\n",
    "\n",
    "print(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
